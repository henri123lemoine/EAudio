{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAudio\n",
    "Generate audio from a URL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import urllib\n",
    "import pyttsx3\n",
    "import os\n",
    "import winreg\n",
    "from bs4 import BeautifulSoup, NavigableString"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "urls_to_test = [\n",
    "        \"https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill\", # Lesswrong\n",
    "        \"https://forum.effectivealtruism.org/posts/oGdCtvuQv4BTuNFoC/good-things-that-happened-in-ea-this-year\", # EA Forum\n",
    "        \"https://www.alignmentforum.org/posts/JSkqkgYcyYt8oHsFi/large-language-models-can-provide-normative-assumptions-for\", # Alignment Forum\n",
    "        \"https://www.gwern.net/Melatonin\", # Gwern\n",
    "        \"https://astralcodexten.substack.com/p/sorry-i-still-think-i-am-right-about\", # Substack\n",
    "        \"https://arbital.com/p/bayes_rule/\", # Arbital\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://astralcodexten.substack.com/p/sorry-i-still-think-i-am-right-about\"\n",
    "url = \"https://astralcodexten.substack.com/p/how-do-ais-political-opinions-change\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    def __init__(self, url: str):\n",
    "        self.url = url\n",
    "        self.parsed_link = urllib.parse.urlparse(url)\n",
    "\n",
    "        headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
    "        self.page = requests.get(url, headers=headers).text\n",
    "        self.soup = BeautifulSoup(self.page, 'html.parser')\n",
    "\n",
    "        self.title = None\n",
    "        self.author = None\n",
    "        self.date = None\n",
    "        self.text = None\n",
    "\n",
    "        if self.parsed_link.netloc == 'www.lesswrong.com' or self.parsed_link.netloc == 'www.alignmentforum.org' or self.parsed_link.netloc == 'forum.effectivealtruism.org':\n",
    "            self.get_text_lw_eaf_af()\n",
    "        elif self.parsed_link.netloc == 'www.gwern.net':\n",
    "            self.get_info_gwern()\n",
    "        elif '.'.join(self.parsed_link.netloc.split('.')[1:]) == 'substack.com':\n",
    "            self.get_info_substack()\n",
    "        elif self.parsed_link.netloc == 'arbital.com':\n",
    "            self.get_info_arbital()\n",
    "        else:\n",
    "            raise ValueError(f'URL {url} is not yet supported.')\n",
    "        \n",
    "        outro = f\"\"\"This was '{self.title}' by {self.author}. You can find the original post at {self.url}.\"\"\"\n",
    "        self.text = f\"{self.text}\\n\\n{outro}\"\n",
    "\n",
    "    def get_text_lw_eaf_af(self):\n",
    "        self.title = self.soup.find(\"a\", {\"class\": \"PostsPageTitle-link\"}).text\n",
    "        self.author = re.sub(r',([^,]*)$', r' and\\1', \", \".join([t.text for t in self.soup.find_all(\"span\", {\"class\": \"PostsAuthors-authorName\"})]))\n",
    "        self.date = self.soup.find(\"span\", {\"class\": \"PostsPageDate-date\"}).text\n",
    "        tags = \"Tags: \" + \", \".join([t.text for t in self.soup.find(\"span\", {\"class\": \"FooterTagList-root\"}).find_all(\"span\", {\"class\": \"FooterTag-name\"})])\n",
    "        raw_text = self.soup.find(\"div\", {\"class\": \"PostsPage-postContent instapaper_body ContentStyles-base content ContentStyles-postBody\"}).find_all([\"p\", \"li\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "        text = \"\\n\".join([t.text for t in raw_text])\n",
    "\n",
    "        self.title = f\"{self.title}\".replace(\":\", \"-\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"<\", \"(\").replace(\">\", \")\").replace(\"*\", \"\").replace(\"?\", \"\").replace(\"|\", \"#\")\n",
    "        self.text = f\"{self.title}, by {self.author}\\nPosted on the {self.date}\\nTags: {tags}\\n\\n{text}\"\n",
    "\n",
    "    def get_info_gwern(self):\n",
    "        self.title = self.soup.h1.string        \n",
    "        self.author = \"Gwern Branwen\"\n",
    "        dates = self.soup.find(id=\"page-date-range\").text.split('â€“')\n",
    "        start_date = datetime.strptime(dates[0], '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(dates[1], '%Y-%m-%d')\n",
    "        start_month = start_date.strftime('%B')\n",
    "        end_month = end_date.strftime('%B')\n",
    "        self.date = f\"Written from {start_month} {start_date.day}, {start_date.year}, to {end_month} {end_date.day}, {end_date.year}.\"\n",
    "\n",
    "        body = self.soup.find(\"div\", {\"id\": \"markdownBody\", \"class\": \"markdownBody\"})\n",
    "\n",
    "        bold_elements = body.find_all(['b', 'strong'])\n",
    "        italic_elements = body.find_all(['i', 'em'])\n",
    "        sup_tags = body.find_all('sup')\n",
    "        blockquotes = body.find_all('blockquote')\n",
    "\n",
    "        to_delete = body.find_all(id=['see-also', 'external-links', 'appendix', 'appendices', 'footnotes', 'backlinks-section', 'link-bibliography-section', 'similars-section']) + body.find_all('noscript')\n",
    "\n",
    "        for element in bold_elements:\n",
    "            element.replace_with('**' + element.text + '**')\n",
    "        for element in italic_elements:\n",
    "            element.replace_with('*' + element.text + '*')\n",
    "        for element in sup_tags:\n",
    "            element.decompose()\n",
    "        for i,element in enumerate(blockquotes):\n",
    "            if i == 0:\n",
    "                element.replace_with('Abstract' + element.text)\n",
    "                continue\n",
    "            element.replace_with('Quote.' + element.text + 'Unquote.')\n",
    "        for element in to_delete:\n",
    "            element.decompose()\n",
    "            \n",
    "        self.title = f\"{self.title}\".replace(\":\", \"-\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"<\", \"(\").replace(\">\", \")\").replace(\"*\", \"\").replace(\"?\", \"\").replace(\"|\", \"#\")\n",
    "        self.text = f\"{self.title}, by {self.author}\\nDate range: {self.date}\\n\\n{body.get_text()}\"\n",
    "\n",
    "    def get_info_substack(self):\n",
    "        self.title = self.soup.title.string\n",
    "        subtitle = self.soup.find(\"h3\", {\"class\": \"subtitle\"}).string\n",
    "        self.author = \"Scott Alexander\"\n",
    "        self.date = self.soup.time.string\n",
    "        body = self.soup.find(\"div\", {\"class\": \"body markup\"})\n",
    "\n",
    "        bold_elements = body.find_all(['b', 'strong'])\n",
    "        italic_elements = body.find_all(['i', 'em'])\n",
    "        sup_tags = body.find_all('sup')\n",
    "        imgs = body.find_all('picture')\n",
    "        code = body.find_all('code')\n",
    "        titles = body.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        blockquotes = body.find_all('blockquote')\n",
    "\n",
    "        for element in bold_elements:\n",
    "            element.replace_with('**' + element.text + '**')\n",
    "        for element in italic_elements:\n",
    "            element.replace_with('*' + element.text + '*')\n",
    "        for element in sup_tags:\n",
    "            element.decompose()\n",
    "        for element in imgs:\n",
    "            element.replace_with(\"\\n[IMAGE ATTACHED]\\n\")\n",
    "        for element in code:\n",
    "            element.replace_with(\"\\n`\" + element.text + \"`\\n\")\n",
    "        for element in titles:\n",
    "            element.replace_with(\"\\n\" + element.text + \"\\n\")\n",
    "        for element in blockquotes:\n",
    "            element.replace_with('Quote.' + element.text + 'Unquote.')\n",
    "\n",
    "        self.title = f\"{self.title}\".replace(\":\", \"-\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"<\", \"(\").replace(\">\", \")\").replace(\"*\", \"\").replace(\"?\", \"\").replace(\"|\", \"#\")\n",
    "        self.text = f\"{self.title}\\n{subtitle}\\nby {self.author}\\nPosted on {self.date}\\n\\n{body.get_text()}\"\n",
    "\n",
    "    def get_info_arbital(self):\n",
    "        raise NotImplementedError(\"Arbital is not supported yet.\")\n",
    "            \n",
    "    def __str__(self):\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio:\n",
    "    def __init__(self, url: str, save_path: str=None, voice: int=0, speed: int=100) -> None:\n",
    "        \"\"\"\n",
    "        :param url: the URL of the text to be read\n",
    "        :param save_path: the path to save the audio file to; if None, it saves in the download folder\n",
    "        :param voice: the voice to use (enter an int if you know which to use, and manually in the terminal if you don't)\n",
    "        :param speed: the speed to read the text at (100 is standard)\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = winreg.QueryValueEx(winreg.OpenKey(winreg.HKEY_CURRENT_USER, r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"), \"{374DE290-123F-4565-9164-39C4925E467B}\")[0]\n",
    "\n",
    "        self.url = url\n",
    "        self.save_path = save_path\n",
    "        self.voice = voice\n",
    "        self.speed = speed\n",
    "\n",
    "        self.text = Text(url)\n",
    "        self.engine = self.set_engine()\n",
    "        self.save_audio()\n",
    "\n",
    "    def set_engine(self):\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[self.voice].id)\n",
    "        engine.setProperty('rate', self.speed)\n",
    "        return engine\n",
    "\n",
    "    def save_audio(self):\n",
    "        self.engine.save_to_file(self.text.text, f\"{self.save_path}/{self.text.title}.mp3\")\n",
    "        self.engine.runAndWait()\n",
    "        self.engine.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Simulators seminar sequence] #1 Background & shared assumptions\n",
      "Audio saved successfully\n"
     ]
    }
   ],
   "source": [
    "audio = Audio(\"https://www.lesswrong.com/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared\", 'data', voice=1, speed=300)\n",
    "print(audio.text.title)\n",
    "print(\"Audio saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
